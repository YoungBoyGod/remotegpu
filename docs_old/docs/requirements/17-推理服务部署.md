# 推理服务部署

> 所属模块：模块 8 - 推理服务模块
>
> 功能编号：8.1
>
> 优先级：P2（可选）

---

## 1. 功能概述

### 1.1 功能描述

推理服务部署功能支持用户将训练好的模型部署为 REST API 服务，提供在线推理能力，支持自动扩缩容、负载均衡和版本管理。

### 1.2 业务价值

- ✅ 快速部署模型为 API 服务
- ✅ 支持多种推理框架（TensorFlow Serving、TorchServe、ONNX Runtime）
- ✅ 自动扩缩容和负载均衡
- ✅ 版本管理和灰度发布

---

## 2. 核心功能

### 2.1 支持的推理框架

| 框架 | 说明 | 适用场景 |
|------|------|---------|
| TensorFlow Serving | TensorFlow 模型推理 | TensorFlow 模型 |
| TorchServe | PyTorch 模型推理 | PyTorch 模型 |
| ONNX Runtime | ONNX 格式模型推理 | 跨框架模型 |
| Triton Inference Server | NVIDIA 推理服务器 | 高性能推理 |

### 2.2 部署模式

```yaml
部署模式:
  单实例部署:
    - 适用于低并发场景
    - 资源占用少

  多实例部署:
    - 支持负载均衡
    - 高可用性

  自动扩缩容:
    - 根据负载自动调整实例数
    - 成本优化
```

---

## 3. 数据模型

```sql
CREATE TABLE inference_services (
    id BIGSERIAL PRIMARY KEY,
    uuid VARCHAR(64) UNIQUE NOT NULL,
    customer_id BIGINT NOT NULL,
    workspace_id BIGINT,

    -- 服务信息
    name VARCHAR(256) NOT NULL,
    description TEXT,

    -- 模型信息
    model_path VARCHAR(512) NOT NULL,
    model_format VARCHAR(32), -- 'tensorflow', 'pytorch', 'onnx'
    framework VARCHAR(32) NOT NULL,

    -- 部署配置
    replicas INT DEFAULT 1,
    min_replicas INT DEFAULT 1,
    max_replicas INT DEFAULT 10,

    -- 资源配置
    cpu_cores INT DEFAULT 2,
    memory_gb INT DEFAULT 4,
    gpu_count INT DEFAULT 0,

    -- 访问信息
    endpoint_url VARCHAR(512),
    api_key VARCHAR(128),

    -- 状态
    status VARCHAR(32) DEFAULT 'pending',

    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),

    FOREIGN KEY (customer_id) REFERENCES customers(id)
);
```

---

## 4. 部署实现

### 4.1 服务部署器

```go
// 推理服务部署器
type InferenceDeployer struct {
    k8sClient *kubernetes.Clientset
    db        *gorm.DB
}

// 部署推理服务
func (d *InferenceDeployer) Deploy(req DeployRequest) (*InferenceService, error) {
    // 1. 创建 Deployment
    deployment := d.createDeployment(req)
    _, err := d.k8sClient.AppsV1().Deployments(req.Namespace).Create(
        context.Background(), deployment, metav1.CreateOptions{},
    )
    if err != nil {
        return nil, err
    }

    // 2. 创建 Service
    service := d.createService(req)
    _, err = d.k8sClient.CoreV1().Services(req.Namespace).Create(
        context.Background(), service, metav1.CreateOptions{},
    )
    if err != nil {
        return nil, err
    }

    // 3. 生成 API Key
    apiKey := generateAPIKey()

    // 4. 创建服务记录
    inferenceService := &InferenceService{
        UUID:        generateUUID(),
        CustomerID:  req.CustomerID,
        Name:        req.Name,
        ModelPath:   req.ModelPath,
        Framework:   req.Framework,
        Replicas:    req.Replicas,
        EndpointURL: fmt.Sprintf("https://api.example.com/inference/%s", req.Name),
        APIKey:      apiKey,
        Status:      "deploying",
    }

    if err := d.db.Create(inferenceService).Error; err != nil {
        return nil, err
    }

    return inferenceService, nil
}
```

---

## 5. API 接口

### 5.1 部署推理服务

```go
POST /api/inference/services
Body: {
  "name": "resnet50-classifier",
  "model_path": "/models/resnet50",
  "framework": "torchserve"
}

Response: {
  "service_id": "svc-abc123",
  "status": "deploying"
}
```

---

## 6. 测试用例

| 用例 | 场景 | 预期结果 |
|------|------|---------|
| TC-01 | 部署推理服务 | 服务部署成功 |
| TC-02 | 调用推理 API | 返回预测结果 |

---

**文档版本：** v1.0
**创建日期：** 2026-01-26
